# ğŸ¤– ML Model Specifications & Hyperparameters

## ğŸ“Š Model Ã–zeti

### **Model Tipi: REGRESSION âœ…**

**AmaÃ§:** Form skorunu 0-100 arasÄ± sÃ¼rekli deÄŸer olarak tahmin etmek

**Neden Regression?**
- Form skoru sÃ¼rekli bir deÄŸer (0-100)
- Classification deÄŸil (binary veya multi-class deÄŸil)
- Skorun kendisini tahmin etmek istiyoruz

---

## ğŸ¯ Mevcut Model Tipleri

### **1. Random Forest Regressor (Ã–nerilen) âœ…**

**Hyperparameters (Åu anki):**
```python
RandomForestRegressor(
    n_estimators=100,      # AÄŸaÃ§ sayÄ±sÄ±
    max_depth=10,          # Maksimum derinlik
    min_samples_split=5,   # Split iÃ§in minimum sample
    random_state=42,       # Reproducibility
    n_jobs=-1             # Paralel iÅŸlem
)
```

**Avantajlar:**
- âœ… Robust (outlier'lara karÅŸÄ± dayanÄ±klÄ±)
- âœ… Feature importance saÄŸlar
- âœ… Overfitting riski dÃ¼ÅŸÃ¼k
- âœ… HÄ±zlÄ± eÄŸitilir

**Dezavantajlar:**
- âŒ Interpretability dÃ¼ÅŸÃ¼k (black box)

---

### **2. Gradient Boosting Regressor**

**Hyperparameters (Åu anki):**
```python
GradientBoostingRegressor(
    n_estimators=100,      # Iteration sayÄ±sÄ±
    max_depth=5,           # Maksimum derinlik
    learning_rate=0.1,     # Ã–ÄŸrenme hÄ±zÄ±
    random_state=42        # Reproducibility
)
```

**Avantajlar:**
- âœ… YÃ¼ksek accuracy
- âœ… Feature importance saÄŸlar

**Dezavantajlar:**
- âŒ YavaÅŸ eÄŸitilir
- âŒ Overfitting riski (tuning gerekir)

---

### **3. Ridge Regression (Baseline)**

**Hyperparameters (Åu anki):**
```python
Ridge(
    alpha=1.0              # Regularization strength
)
```

**Avantajlar:**
- âœ… HÄ±zlÄ± (linear model)
- âœ… Interpretable (coefficient'lar)
- âœ… Baseline iÃ§in uygun

**Dezavantajlar:**
- âŒ Non-linear pattern'leri yakalayamaz
- âŒ DÃ¼ÅŸÃ¼k accuracy (baseline iÃ§in)

---

## ğŸ“ˆ Performance Evaluation Metrics

### **1. Mean Squared Error (MSE)**

**FormÃ¼l:**
```
MSE = (1/n) * Î£(y_true - y_pred)Â²
```

**Yorumlama:**
- DÃ¼ÅŸÃ¼k = Ä°yi
- Birim: ScoreÂ² (Ã¶rnek: 25.0 = Â±5 puan hatasÄ±)
- BÃ¼yÃ¼k hatalara daha fazla aÄŸÄ±rlÄ±k verir

**Hedef:** < 50 (ortalama Â±7 puan hatasÄ±)

---

### **2. Mean Absolute Error (MAE)**

**FormÃ¼l:**
```
MAE = (1/n) * Î£|y_true - y_pred|
```

**Yorumlama:**
- DÃ¼ÅŸÃ¼k = Ä°yi
- Birim: Score (Ã¶rnek: 5.0 = ortalama 5 puan hatasÄ±)
- TÃ¼m hatalara eÅŸit aÄŸÄ±rlÄ±k verir

**Hedef:** < 7 (ortalama 7 puan hatasÄ±)

---

### **3. RÂ² Score (Coefficient of Determination)**

**FormÃ¼l:**
```
RÂ² = 1 - (SS_res / SS_tot)
```

**Yorumlama:**
- 0-1 arasÄ± deÄŸer
- 1.0 = MÃ¼kemmel (tam tahmin)
- 0.0 = Baseline (ortalama deÄŸer kadar iyi)
- < 0 = Baseline'dan kÃ¶tÃ¼

**Hedef:** > 0.85 (variance'Ä±n %85'ini aÃ§Ä±klÄ±yor)

---

## ğŸ”§ Hyperparameter Tuning (Åu anki durum: YOK âŒ)

### **Mevcut Durum:**
- âŒ Hyperparameter'lar **sabit** (tuning yok)
- âŒ GridSearch/RandomSearch yok
- âŒ Cross-validation yok (sadece train/test split)

### **Ã–nerilen Ä°yileÅŸtirme:**

#### **1. GridSearchCV ile Hyperparameter Tuning**

```python
from sklearn.model_selection import GridSearchCV

# Random Forest iÃ§in
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid,
    cv=5,  # 5-fold cross-validation
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_
```

**Avantajlar:**
- âœ… Optimal hyperparameter'larÄ± bulur
- âœ… Cross-validation ile robust
- âœ… Overfitting riskini azaltÄ±r

---

#### **2. RandomizedSearchCV (Daha HÄ±zlÄ±)**

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# Random Forest iÃ§in
param_dist = {
    'n_estimators': randint(50, 300),
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10)
}

random_search = RandomizedSearchCV(
    RandomForestRegressor(random_state=42),
    param_dist,
    n_iter=50,  # 50 kombinasyon dene
    cv=5,
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    random_state=42
)

random_search.fit(X_train, y_train)
best_model = random_search.best_estimator_
```

**Avantajlar:**
- âœ… Daha hÄ±zlÄ± (tÃ¼m kombinasyonlarÄ± denemez)
- âœ… Yeterince iyi sonuÃ§lar verir

---

#### **3. K-Fold Cross-Validation**

**Åu anki:** Sadece train/test split (80/20)

**Ã–nerilen:** 5-Fold Cross-Validation
```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(
    model,
    X, y,
    cv=5,
    scoring='neg_mean_absolute_error'
)

print(f"CV MAE: {-scores.mean():.2f} (Â±{scores.std():.2f})")
```

**Avantajlar:**
- âœ… Daha robust performance estimation
- âœ… Overfitting tespiti
- âœ… TÃ¼m veriyi kullanÄ±r (daha iyi evaluation)

---

## ğŸ“Š Mevcut vs Ã–nerilen

| Ã–zellik | Mevcut Durum | Ã–nerilen |
|---------|-------------|----------|
| **Hyperparameter Tuning** | âŒ Yok (sabit deÄŸerler) | âœ… GridSearch/RandomSearch |
| **Cross-Validation** | âŒ Yok (sadece train/test) | âœ… 5-Fold CV |
| **Performance Metrics** | âœ… MSE, MAE, RÂ² | âœ… MSE, MAE, RÂ² + CV scores |
| **Model Selection** | âœ… 3 tip var | âœ… + Hyperparameter tuning |

---

## ğŸ¯ Ã–nerilen Hyperparameter Ranges

### **Random Forest:**
```python
{
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}
```

### **Gradient Boosting:**
```python
{
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'min_samples_split': [2, 5, 10],
    'subsample': [0.8, 0.9, 1.0]
}
```

### **Ridge:**
```python
{
    'alpha': [0.1, 0.5, 1.0, 5.0, 10.0, 50.0]
}
```

---

## ğŸš€ Implementation Plan

### **1. Hyperparameter Tuning Ekle**

```python
# ml_trainer.py iÃ§ine ekle
def tune_hyperparameters(self, X, y, cv=5):
    """Tune hyperparameters using GridSearchCV."""
    if self.model_type == "random_forest":
        param_grid = {...}
        grid_search = GridSearchCV(...)
        grid_search.fit(X, y)
        self.model = grid_search.best_estimator_
        return grid_search.best_params_
```

### **2. Cross-Validation Ekle**

```python
def train_with_cv(self, samples, cv=5, verbose=True):
    """Train with cross-validation."""
    X, y = self.prepare_features(samples)
    
    # Cross-validation scores
    cv_scores = cross_val_score(
        self.model, X, y,
        cv=cv,
        scoring='neg_mean_absolute_error'
    )
    
    # Train on full dataset
    self.model.fit(X, y)
    
    return cv_scores
```

### **3. Performance Reporting Ä°yileÅŸtir**

```python
def evaluate(self, X_test, y_test):
    """Comprehensive evaluation."""
    y_pred = self.model.predict(X_test)
    
    metrics = {
        'mse': mean_squared_error(y_test, y_pred),
        'mae': mean_absolute_error(y_test, y_pred),
        'r2': r2_score(y_test, y_pred),
        'rmse': np.sqrt(mean_squared_error(y_test, y_pred))
    }
    
    return metrics
```

---

## ğŸ“ˆ Beklenen Performans

### **Minimum (Baseline):**
- **MAE:** < 10 puan
- **RÂ²:** > 0.70
- **MSE:** < 100

### **Ä°yi:**
- **MAE:** < 7 puan
- **RÂ²:** > 0.85
- **MSE:** < 50

### **MÃ¼kemmel:**
- **MAE:** < 5 puan
- **RÂ²:** > 0.90
- **MSE:** < 25

---

## âœ… Checklist

- [x] Model tipi belirlendi (REGRESSION)
- [x] 3 model tipi eklendi (RF, GB, Ridge)
- [x] Performance metrics eklendi (MSE, MAE, RÂ²)
- [ ] Hyperparameter tuning eklenecek
- [ ] Cross-validation eklenecek
- [ ] Performance reporting iyileÅŸtirilecek

---

## ğŸ¯ SonuÃ§

**Mevcut Durum:**
- âœ… Regression modeli var
- âœ… 3 farklÄ± algoritma var
- âœ… Temel performance metrics var
- âŒ Hyperparameter tuning yok
- âŒ Cross-validation yok

**Ã–nerilen Ä°yileÅŸtirmeler:**
1. GridSearch/RandomSearch ekle
2. 5-Fold cross-validation ekle
3. Performance reporting iyileÅŸtir
4. Hyperparameter ranges optimize et

